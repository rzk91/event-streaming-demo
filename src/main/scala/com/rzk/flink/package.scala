package com.rzk

import cats.kernel.{Eq, Monoid}
import cats.syntax.monoid._
import com.ariskk.flink4s.{DataStream, StreamExecutionEnvironment}
import com.rzk.common.TimestampedObject
import com.rzk.runconfig.RunConfiguration.{CheckpointingConfig, KafkaConfig}
import com.rzk.util.implicits.enrichDataStream
import io.circe.{Decoder, Encoder}
import org.apache.flink.api.common.eventtime.WatermarkStrategy
import org.apache.flink.api.common.restartstrategy.RestartStrategies
import org.apache.flink.api.common.time.Time
import org.apache.flink.api.common.typeinfo.TypeInformation
import org.apache.flink.connector.kafka.sink.{KafkaRecordSerializationSchema, KafkaSink}
import org.apache.flink.connector.kafka.source.KafkaSource
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer
import org.apache.flink.streaming.api.datastream.DataStreamSink
import org.apache.kafka.clients.consumer.{ConsumerConfig, OffsetResetStrategy}

package object flink {

  def defaultExecutionEnvironment(
    parallelism: Int = 1,
    checkpointing: CheckpointingConfig
  ): StreamExecutionEnvironment = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    env.javaEnv.setParallelism(parallelism)
    env.javaEnv.getConfig.enableObjectReuse()
    env.javaEnv.getConfig.enableAutoGeneratedUIDs()

    env.javaEnv.setRestartStrategy(RestartStrategies.failureRateRestart(3, Time.minutes(10), Time.seconds(10)))
    checkpointing.setFlinkEnvCheckpointing(env)
  }

  def flinkKafkaSource[A <: TimestampedObject: Decoder: Eq: Monoid: TypeInformation](
    bot: String,
    kafkaConfig: KafkaConfig,
    checkpointingInterval: Long,
    runningLocally: Boolean,
    removeEmpty: Boolean = true
  )(implicit env: StreamExecutionEnvironment): DataStream[A] = {
    val consumer = KafkaSource
      .builder[A]
      .setBootstrapServers(kafkaConfig.bootstrapServers.mkString(","))
      .setTopics(kafkaConfig.topics.events)
      .setProperties(kafkaConfig.auth.asProperties)
      .setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, kafkaConfig.offset.enableOffsetCommit.toString)
      .setProperty(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, checkpointingInterval.toString)
      .setGroupId(kafkaConfig.groupId(consumer = true, "flink"))
      .setValueOnlyDeserializer(new JsonDeserializer[A])
      .setStartingOffsets {
        if (runningLocally && kafkaConfig.offset.startFromEarliest) OffsetsInitializer.earliest()
        else OffsetsInitializer.committedOffsets(OffsetResetStrategy.EARLIEST)
      }
      .build()

    DataStream {
      env.javaEnv
        .fromSource(
          consumer,
          WatermarkStrategy
            .forMonotonousTimestamps[A]
            .withTimestampAssigner(new SimpleTimestampAssigner[A]),
          s"Event source $bot",
          implicitly[TypeInformation[A]]
        )
    }.filterNot(_.isEmpty && removeEmpty)
  }

  def flinkKafkaSink[A: Encoder: TypeInformation](
    output: DataStream[A],
    kafkaConfig: KafkaConfig
  ): DataStreamSink[A] = {
    val producer = KafkaSink
      .builder[A]
      .setBootstrapServers(kafkaConfig.bootstrapServers.mkString(","))
      .setKafkaProducerConfig(kafkaConfig.auth.asProperties)
      .setTransactionalIdPrefix(kafkaConfig.groupId(consumer = false, "flink"))
      .setRecordSerializer(
        KafkaRecordSerializationSchema
          .builder[A]
          .setTopic(kafkaConfig.topics.findings)
          .setValueSerializationSchema(new JsonSerializer[A])
          .build()
      )
      .setDeliverGuarantee(kafkaConfig.deliveryGuarantee)
      .build()

    output.toSink(producer)
  }
}
